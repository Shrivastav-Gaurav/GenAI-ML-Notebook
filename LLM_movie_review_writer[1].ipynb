{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrivastav-Gaurav/GenAI-ML-Notebook/blob/main/LLM_movie_review_writer%5B1%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYXu9IY2Pvd-"
      },
      "source": [
        "In this notebook, we use the 3.8B parameter [Phi-3 model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) to write move reviews.\n",
        "\n",
        "We will also take a closer look at the inputs and outputs of the tokenizer and the LLM, to understand what is happening under the hood when we call a LLM text generation API.\n",
        "\n",
        "We also look at the effect of `temperature`, a common parameter used to control LLM text generation randomness. Higher temperature gives more random/creative outputs, while lower, near zero gives less varied outputs. There is also a \"greedy\" text generation strategy: under this strategy, the vocab token with the highest next token probability will always be selected, making the generation deterministic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TOz1Me6C-EX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "transformers.utils.logging.set_verbosity_error()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download phi-3 model and its tokenizer from hugging face\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "phi3_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "                                             torch_dtype=\"auto\",\n",
        "                                             trust_remote_code=True\n",
        "                                             )\n",
        "phi3_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "1DxyN4NuN90X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9zeptv1f31z"
      },
      "outputs": [],
      "source": [
        "# Our text generation \"API\".\n",
        "def llm_generate_text(model, tokenizer, prompt, temperature=0.8, max_new_tokens=256):\n",
        "  tokenized_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "  tokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  predicted_token_ids = model.generate(\n",
        "      **tokenized_inputs,\n",
        "      do_sample=True,\n",
        "      temperature=temperature,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "  )[0]\n",
        "\n",
        "  print(tokenizer.decode(predicted_token_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate movie review using phi3 model\n",
        "\n",
        "* We ask the model to write a movie review starting with \"I\".\n",
        "* The phi3 model expects chat formatted input. We use `tokenizer.apply_chat_template` to easily do this.\n",
        "\n",
        "It's easy to see that phi3's movie reviews are vastly better than our previous bi-gram and 4-gram models. Importantly, we did not need to train the phi3 model either: the model has gone through sufficient prior training to satisfy our request."
      ],
      "metadata": {
        "id": "MfNylVqiQykS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Write a movie review. The movie should be an actual movie. The review should start with the word \"I\".'\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "phi3_prompt = phi3_tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "print(phi3_prompt)\n",
        "\n",
        "llm_generate_text(phi3_model, phi3_tokenizer, phi3_prompt)"
      ],
      "metadata": {
        "id": "LdVlHAE0PRlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's look at the `llm_generate_text` function line by line.\n",
        "```\n",
        "def llm_generate_text(model, tokenizer, prompt, temperature=0.8, max_new_tokens=256):\n",
        "  tokenized_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "  tokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  predicted_token_ids = model.generate(\n",
        "      **tokenized_inputs,\n",
        "      do_sample=True,\n",
        "      temperature=temperature,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "  )[0]\n",
        "\n",
        "  print(tokenizer.decode(predicted_token_ids))\n",
        "```"
      ],
      "metadata": {
        "id": "tqC5rFP92kh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenize text input\n",
        "print(f'prompt = {phi3_prompt}')\n",
        "print()\n",
        "\n",
        "print('Tokenizing the prompt......')\n",
        "phi3_tokens = phi3_tokenizer(phi3_prompt, return_tensors=\"pt\")\n",
        "print(phi3_tokens)"
      ],
      "metadata": {
        "id": "6YJfCLRy21_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title The LLM generates token ids\n",
        "phi3_tokens = {k: v.to(device) for k, v in phi3_tokens.items()}\n",
        "phi3_model = phi3_model.to(device)\n",
        "\n",
        "predicted_phi3_token_ids = phi3_model.generate(\n",
        "      **phi3_tokens,\n",
        "      do_sample=True,\n",
        "      temperature=0.9,\n",
        "      max_new_tokens=16,\n",
        "  )\n",
        "predicted_phi3_token_ids"
      ],
      "metadata": {
        "id": "8mZVnm0q34ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Translate generated token ids back to text\n",
        "phi3_tokenizer.decode(predicted_phi3_token_ids[0])"
      ],
      "metadata": {
        "id": "t0eZxfG644yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Controling text generation randomness with temperature\n",
        "for _ in range(3):\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print(f'temperature = 1e-9')\n",
        "  llm_generate_text(phi3_model, phi3_tokenizer, phi3_prompt, temperature=1e-9, max_new_tokens=16)\n",
        "print('###################################################################')\n",
        "\n",
        "for _ in range(3):\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print(f'temperature = 1.0')\n",
        "  llm_generate_text(phi3_model, phi3_tokenizer, phi3_prompt, temperature=1.0, max_new_tokens=16)\n",
        "print('###################################################################')\n",
        "\n",
        "for _ in range(3):\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print(f'temperature = 10.0')\n",
        "  llm_generate_text(phi3_model, phi3_tokenizer, phi3_prompt, temperature=10.0, max_new_tokens=16)\n",
        "print('###################################################################')"
      ],
      "metadata": {
        "id": "Ku4dG8Ok55Jv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}